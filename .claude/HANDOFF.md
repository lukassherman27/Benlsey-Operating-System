# Agent Handoff

**Updated:** 2025-12-19 (Business Context + UX Overhaul)

---

## 1. What Is This System?

**BENSLEY Design Studios (BDS)** - World-renowned luxury hospitality design firm, Bangkok.

| Person | Role |
|--------|------|
| **Bill Bensley** | Founder, Creative Director (40+ years) |
| **Brian Petrie** | Director/CEO (30+ years) |
| **Lukas Sherman** | Business Development, Technology |

**This platform is Bill's Personal Operating System** covering:
- **BDS** - Core design business (full tracking)
- **Shinta Mani Hotels** - Bill part-owner (emails, some projects)
- **SM Foundation** - Bill's charity (emails only)
- **Personal** - Investments, art, press (emails only)

**Goal:** "Bensley Brain" - AI that knows everything about the business.

---

## 2. Team Structure

### Leadership
- Bill Bensley - Creative Director
- Brian Petrie - CEO/Director

### Directors/Heads (Each has their own team)
| Name | Discipline |
|------|------------|
| Ouant | Interior |
| Aood | Interior |
| Spot | Architecture |
| Putu | Bali Director |
| Suwit | Architecture |
| Gawow | Architecture |
| Mu | Landscape |

### Disciplines
- Architecture
- Interior Design
- Landscape
- Graphics Design
- Artwork

### Staff
- 98 total team members
- Directors have teams under them

---

## 3. Project Accountability

For each PROJECT + PHASE, we need:
- Point of Contact - Architecture
- Point of Contact - Interior
- Point of Contact - Landscape
- (Graphics, Artwork as needed)

This enables routing tasks to the right person.

---

## 4. Proposal → Project Conversion

```
PROPOSAL                    →    PROJECT
─────────────────────────────────────────
Tracking, negotiating       →    Contract signed (both parties)
                            →    Mobilization invoiced
                            →    Mobilization PAID
                            →    = Active Project

Project code stays the same (e.g., 25 BK-033)
Moves from proposals table to projects table
```

### Timeline Continuity (CRITICAL)
The full history must be preserved:
- When a proposal was created
- All status changes during proposal phase
- When contract was signed
- When it became an active project
- All project phase transitions

**Analytics need:** "Show me the timeline from first contact to active project to completion."

Email links can exist to both proposal AND project - this is fine. The point is having the complete history, not table purity.

---

## 5. Proposal Statuses (Final List)

| Status | Meaning |
|--------|---------|
| **First Contact** | Initial inquiry |
| **Meeting Held** | Had discussions |
| **NDA Signed** | Confidentiality agreed |
| **Proposal Prep** | Writing the proposal |
| **Proposal Sent** | Fee sent, awaiting response |
| **Negotiation** | Active back-and-forth |
| **On Hold** | Client paused |
| **Dormant** | Months of silence (not lost) |
| **Lost** | Went to competitor |
| **Declined** | We said no |
| **Contract Signed** | Won → converts to Project |

---

## 6. Project Phases

| Phase | Code | Notes |
|-------|------|-------|
| Mobilization | MOB | Week 1 |
| Concept Design | CD | 8-10 weeks |
| Schematic Design | SD | Sometimes merged into DD |
| Design Development | DD | |
| Construction Documents | CDocs | (not CD to avoid confusion) |
| Construction Observation | CO | Until end |

**Typical project:** ~2 years contract term, but delays are common

---

## 6.1 Business Rules (Dec 2025)

### Project Types
- **Resort hotels** (primary)
- **Mixed-use commercial** (only when attached to resorts)
- **High-end residential** (palaces, villas)
- **Boutique hotels** (mountain, urban, anywhere)

### Minimum Fee
- **$1M USD** minimum (exceptions for extensions/additional services)

### Deliverables
- Drawings, renderings, perspectives, cutouts
- Each phase has specific deliverables with sub-tasks
- Example: "50% DD Presentation" contains: interior lobby, ground floor, wall details, etc.

### PM Tracking Needs
- Deliverables with due dates
- Sub-tasks within deliverables
- Who is doing what (resource allocation)
- Scheduling / team capacity
- RFIs (2-day turnaround during construction)

### RFIs (Request for Information)
- From client during construction
- Usually need material specs, drawing clarifications
- **~2 day response time required** so construction can continue

### Financial Tracking (Bill/Brian/Accounting)
- Payments vs Progress: "They haven't paid but we're this far along → slow down"
- Real-time invoice status
- Outstanding amounts

---

## 6.2 Daily Workflow (What Bill Needs)

When Bill looks at the system each day:
1. **Meetings today** - What's scheduled
2. **Proposals needing a nudge** - Based on context
3. **Things to send out** - Deliverables, proposals
4. **Tasks for today/this week**

### Follow-up Logic (CRITICAL)
**NOT just "10 days no response = follow up"**

Follow-up depends on CONTEXT from correspondence:
- If they said "we'll send you the design brief" → follow up asking for it
- If we sent proposal and they said "give us time to check with CEO" → wait longer
- If we sent proposal and no response for 10 days → probably time to nudge

### Ball in Court
Based on correspondence - who said they would do something next:
- **Us**: "We'll send the proposal tomorrow"
- **Them**: "We'll send you the design brief"
- Move ball on each action completion

### At-a-Glance View
1. **Is everything okay?** - Quick health check
2. **Drill down for story** - Full proposal history
3. **Summary of entire history** - AI-generated narrative
4. **Scope details** - Fee breakdown, what's included

---

## 6.3 Project Selection

Not every inquiry becomes a project:
- Too small / too busy
- Not exciting enough
- Too basic (want special, brand-enhancing work)
- Bad fit (philosophies don't align)

Projects should **enhance the Bensley brand**.

---

## 7. Email Channels (Planned)

| Account | Purpose | Status |
|---------|---------|--------|
| `lukas@bensley.com` | Proposals, BD, general | ✅ Active |
| `bill@bensley.com` | Bill's inbox | Planned |
| `projects@bensley.com` | Client correspondence (PMs CC this) | This week |
| `invoices@bensley.com` | Payment tracking | Planned |
| `dailywork@bensley.com` | Staff daily submissions | Planned |
| `scheduling@bensley.com` | PM scheduling | Planned |

### How Each Channel Works

**projects@** - PMs CC this on client emails. Clients told to send RFIs here. All correspondence for a project in one place.

**dailywork@** - Staff send daily work + attachments (sketches, renders). Bill/Brian review and comment. Comments become tracked tasks.

**invoices@** - Accountant sends invoices to clients from here. Payment tracking.

**scheduling@** - Internal PM coordination. Mix of internal + external scheduling.

---

## 8. Categorization Strategy

### Layer 1: Primitive Rules (Auto)
- `@bensley.com` → Internal
- Known contact + project mapping → Link to that project
- Email from `projects@` → It's project-related

### Layer 2: AI Suggestions (Within project)
Once linked to a project, AI suggests sub-category:
- RFI
- General correspondence
- Scheduling
- Site visit
- Drawing sharing
- Consultant coordination
- Invoice/payment
- Meeting notes

**Phase 1:** Just get everything into the right PROJECT folder
**Phase 2:** Sub-categorization emerges from AI suggestions

---

## 9. Contact Roles

When mapping a contact to a project:
- Client (owner)
- Client Representative
- External Consultant - Civil
- External Consultant - Structural
- External Consultant - MEP
- Operator (hotel brand)
- Contractor
- Other (AI can suggest new roles)

---

## 10. Data Quality Rules

### 9 Lessons Learned
1. **Always verify FK integrity** - Check MIN/MAX on both sides
2. **Code matching > ID matching** - Project codes are stable, IDs drift
3. **Never auto-link** - Always create suggestions for human review
4. **Staging tables** - Never modify production directly
5. **Keep backups** - 7-day retention before dropping
6. **Chronological processing** - Oldest emails first
7. **Check schema first** - Column names vary between tables
8. **Cancelled projects persist** - Train AI to not link to them
9. **Internal categories** - Use INT-* codes for non-project emails

### Column Name Gotchas
| Table | Column |
|-------|--------|
| ai_suggestions | `suggestion_id` (not id), `description` (not reasoning) |
| emails | `sender_email` (not sender) |
| projects | `project_title` (not project_name) |

---

## 11. Project Code Lookups

| Name/Keyword | Code |
|--------------|------|
| La Vie / Sudha / necklace | 25 BK-037 |
| BKC Mumbai / AuroRealty | 25 BK-047 |
| Daimon Sake / Gaggan | 25 BK-048 |
| Wangsimni / jinny | 25 BK-071 |
| Ritz Carlton Nusa Dua | 25 BK-033 |
| Vahine Island / Taha'a | 25 BK-087 |
| Veyo Utah | 25 BK-069 |
| Manali / Badal | 25 BK-050 |
| Taiwan Taitung | 25 BK-078 |
| Equinox Hotels | 25 BK-043 |
| Wynn Marjan Additional Services | 25 BK-039 |
| Costa Rica Resort | 25 BK-038 |
| Sathorn Private Residence | 25 BK-070 |
| Vietnam High Rise Ultra Luxury | 25 BK-064 |
| Akyn Hospitality Da Lat | 25 BK-063 |

**Bill's Nicknames:**
- "jinny" = Jin Young Kim (Wangsimni)
- "necklace" = Sudha Reddy (La Vie)

---

## 12. Suggestion Review Flow

### Current System (Per-Email)
```
AI processes emails → Creates individual suggestions → Human reviews each
Problem: 400+ suggestions/day is unmanageable
```

### Target System (Batched)
```
AI processes emails
       ↓
Groups by sender/domain (not per-email)
       ↓
Creates BATCH suggestions:
  "joe@veyopool.com → 25 BK-069 (45 emails)"
       ↓
Human approves BATCH (1 click = 45 links)
       ↓
Pattern learned for future
```

### Confidence Tiers
| Confidence | Queue | Action |
|------------|-------|--------|
| >0.90 | Quick Approve | Batch by sender, 1-click approve |
| 0.70-0.90 | Review | Batch with sample emails shown |
| 0.50-0.70 | Individual | One suggestion per email |
| <0.50 | Log Only | No suggestion, pattern discovery |

### Suppressed Suggestion Types
- `link_review` - Deleted (0% approval)
- `proposal_status_update` - Suppressed (10% approval)
- `new_contact` - Threshold raised to 0.85 (was 16% approval at 0.70)
- `follow_up_needed` - Converted to weekly report (was creating 150 individual suggestions)

### Internal Email Filter
- @bensley.com emails DO NOT generate email_link suggestions
- Internal emails are already your records, don't need linking

### Weekly Stale Proposals Report
Instead of 150 individual follow_up_needed suggestions, run:
```bash
python scripts/core/generate_stale_proposals_report.py
```
Outputs a single report with proposals grouped by staleness (90+ days, 30-90 days, etc.)

---

## 13. What Questions Get Asked

### Bill asks:
- "What's the status of [proposal]?"
- "What's the fee for [proposal]?"
- "What's the total fee for active projects?"
- "How much has been paid? Outstanding?"
- "What are the deadlines for [project]?"

### You ask via Claude CLI:
- Proposal status updates
- Follow-up needed (who hasn't responded)
- Weekly scheduling summary
- Project context for meetings

---

## 14. Session Protocol

### When Starting
1. Read `.claude/STATUS.md` for numbers
2. Read `docs/roadmap.md` for priorities
3. Run quick DB check:
```sql
SELECT COUNT(*) FROM emails;
SELECT COUNT(*) FROM ai_suggestions WHERE status='pending';
```

### When Ending
1. Update `.claude/STATUS.md` with new numbers
2. Commit and push to GitHub

---

## 15. Don't Do These

1. **Don't auto-link** - Always create suggestions
2. **Don't create tables** without checking if similar exists
3. **Don't delete "orphaned" code** - It's usually CLI tools
4. **Don't assume** - Verify by reading code
5. **Don't rush** - Thoroughness > speed
6. **Don't mention project codes without names** - Always include project name
7. **Don't create random files** - Use existing structure

---

## 16. Key Files

| Purpose | File |
|---------|------|
| Email import | `backend/services/email_importer.py` |
| Email sync | `scripts/core/scheduled_email_sync.py` |
| Transcription | `voice_transcriber/transcriber.py` |
| Context building | `backend/services/context_bundler.py` |
| GPT analysis | `backend/services/gpt_suggestion_analyzer.py` |
| Suggestions | `backend/services/suggestion_writer.py` |
| **Batch suggestions** | `backend/services/batch_suggestion_service.py` |
| Handlers | `backend/services/suggestion_handlers/*.py` |
| Learning | `backend/services/learning_service.py` |
| AI Learning | `backend/services/ai_learning_service.py` |
| CLI review | `backend/services/cli_review_helper.py` |
| **Stale proposals report** | `scripts/core/generate_stale_proposals_report.py` |

---

## 17. Email Category Codes

### Non-Project Categories
| Code | Purpose | Domains/Senders |
|------|---------|-----------------|
| PERS-BILL | Bill's personal matters | Canggu land sale (12,000 sqm), hsfkramer.com, ahp.id |
| PERS-INVEST | Bill's investments | dentons.com, jpmorgan, ryan.padgett, binance |
| SM-WILD | Shinta Mani Wild (Bill's hotel) | shintamani.com, gm.wild@ |
| SM-ANGKOR | Shinta Mani Angkor (Bill's hotel) | Personal, not BDS work |
| SM-FOUNDATION | Shinta Mani Foundation (charity) | shintamanifoundation.org |
| INT-SCHED | Internal scheduling | pakheenai@icloud.com (Aood) |
| SKIP-SPAM | Newsletters, notifications | ghost.io, monday.com, sproutsocial |
| SKIP-AUTO | System notifications | atlassian, pipedrive, pandadoc |

### Shinta Mani Clarification
- **SM Wild, SM Angkor** = Bill's existing hotels. Personal, not BDS work.
- **SM Sabra (Saudi Arabia)** = Active BDS project! Shinta Mani is operator, but Bensley is designing it. Link to project 25 BK-042.

### Category Column Warning
- **USE:** `email_content.category` (correct, rule-based)
- **IGNORE:** `emails.category` (NULLed out, was garbage from bad GPT prompt)

---

## 18. Project Statuses

### Valid Project Statuses
| Status | Meaning |
|--------|---------|
| Active | Currently in progress |
| Completed | Project finished |
| Cancelled | Project cancelled |
| contract_expired | Contract term ended, needs review for renewal |

### Contract Expiry Logic
```
IF current_date > contract_end_date AND status = 'Active'
THEN flag for review → could become:
  - contract_expired (awaiting decision)
  - Completed (work done, closed out)
  - Renewed (new contract signed)
```

---

## 19. Two Staff Tables (Keep Both)

| Table | Purpose | Rows |
|-------|---------|------|
| `staff` | HR/admin view (employment, hierarchy, reports_to) | 100 |
| `team_members` | Scheduling/project view (disciplines, team leads) | 98 |

These serve different purposes. Don't consolidate.

---

## 20. Bug Fixes Applied (Dec 8)

| Bug | Fix | File |
|-----|-----|------|
| Email extraction broken | Added RFC 5322 regex | suggestion_writer.py:67-69 |
| Suggestion duplicates | Added unique index + code check | ai_learning_service.py, ai_suggestions table |
| emails.category garbage | NULLed + updated 5 files to use email_content.category | Multiple |
| follow_up flood | Converted to weekly report | follow_up_agent.py disabled, new script created |

---

## 21. Email Coverage Reality (Dec 9)

**Previous agents were measuring wrong.** They obsessed over "53% proposal linking" when that's not the goal.

### Correct Metrics
| Status | Count | % |
|--------|-------|---|
| Linked to proposal | 1,766 | 47% |
| Linked to project (not proposal) | 317 | 9% |
| Categorized only (no link needed) | 1,348 | 36% |
| **HANDLED TOTAL** | **3,431** | **92%** |
| Truly unhandled | 294 | 8% |

### What "Categorized Only" Means
These 1,348 emails don't NEED proposal/project links:
- internal_scheduling (901) - team calendars
- automated_notification (39) - system emails
- internal (85) - general internal comms

### The Real Gap
Only 294 emails (8%) are truly unhandled. Most are:
- SaaS marketing spam
- Bill's Canggu land sale lawyers
- Misc personal

### Pattern Matching Fixed (Dec 9)
Was broken - `times_used` counter wasn't incrementing in `batch_suggestion_service.py`.
Now working correctly. 142 patterns, 14 with active usage.

### INQUIRY-PENDING Emails (15)
These are potential project inquiries that need human review:
- Magandip Riar (Punjab India project)
- Gunjan Group (India resort)
- Zenesca (consultation request)
- Ayun Group (Raja Ampat)
- Jason Holdsworth (new project)

Query to find them:
```sql
SELECT e.sender_email, e.subject FROM emails e
JOIN email_content ec ON e.email_id = ec.email_id
WHERE ec.category = 'INQUIRY-PENDING';
```

---

## 22. Vision & User Requirements (Dec 10, 2025)

### The Ultimate Goal
**"Everyone at Bensley has AI power"**
- Bill: "What's happening with Nusa Dua?" → gets full context in seconds
- PM: "What RFIs are overdue?" → gets list with context
- System LEARNS from every interaction

### Why This Exists
- Store useful data for automation
- Make operations extremely efficient so Bensley can SCALE
- Free up Bill and Brian's time - business ops is bottlenecked with them
- Currently inefficient: too much sits with leadership

### Target Users & Their Needs

| User | Role | Needs |
|------|------|-------|
| **Bill** | Executive | Instant context on ANY project, pipeline health, financial overview, team workload |
| **Project Managers** | Operations | Their projects, deliverables, RFIs, team assignments |
| **Finance/Admin** | Back office | Invoicing, payments, aging reports, cash flow |

### Dashboard Design
**Role-based views** - NOT one-size-fits-all:
- Bill sees executive overview
- PM sees their projects
- Finance sees invoices

### Killer Features (Must Have)

1. **Instant Context**
   - "What's happening with Nusa Dua?" → full history (emails, meetings, fees, stakeholders, timeline)
   - Under 5 seconds

2. **Never Miss Follow-up**
   - NOT just days-since-contact
   - COMMITMENT tracking: "I said I'd send proposal by Friday" → system reminds
   - Proposal sent → 2 weeks no response → flag for follow-up

3. **Auto-Generated Reports**
   - Weekly pipeline report
   - Cash flow summary
   - PM workload

4. **Voice Transcription**
   - Record meetings → auto-summarize → link to projects → extract action items

### AI Suggestions Display
- **BOTH** inline on pages (yellow card: "No contact in 30 days") AND central queue for batch review
- **Push notifications**: Daily email/Slack with "Today's action items"

### Data Sources to Connect

| Source | Status | Notes |
|--------|--------|-------|
| lukas@bensley.com | ✅ Active | 3,773 emails |
| projects@bensley.com | Planned Jan | PMs CC this |
| bill@bensley.com | Planned Jan | Requires Bill approval |
| invoices@bensley.com | Planned Feb | Payment tracking |
| OneDrive | Planned | Contracts, drawings, proposals |
| Accounting software | Planned | Actual payment data |
| Calendar | Planned | Meetings, deadlines |

### Proposals Focus (What Bill Wants to See)
- Follow-up urgency (WHO to call TODAY and WHY)
- Value breakdown (total pipeline, by status, by country)
- Activity timeline (what happened this week)

### Projects Focus (What PMs Want to See)
- Payments & invoicing (outstanding, aging, cash flow)
- Deliverables & RFIs (what's due, what's overdue)
- Phases: Mobilization → CD → DD → CDocs → CO (NOT alphabetical!)

### PM Software Inspiration
- **Monday.com**: Visual boards, automations, timeline views
- **Notion**: Flexible databases, linked records, wiki-style
- **Asana**: Task hierarchy, workload view, portfolios
- **BUT**: Tailored to Bensley's workflow (proposals, projects, disciplines)

---

## 23. Autonomous Agent Prompts

Use these prompts to spawn specialized agents. Each agent figures out the details themselves - you provide context and guardrails, they diagnose and execute.

### Auditor Agent

**Purpose:** Find bugs, misalignments, broken code, stale data.

**Prompt:**
```
You are auditing the BENSLEY Design Studios Operations Platform.

CONTEXT:
- Backend: FastAPI (port 8000) | Frontend: Next.js (port 3002) | DB: SQLite
- Database: database/bensley_master.db (~108 tables)
- Key files: backend/services/*.py, backend/api/routers/*.py, frontend/src/**
- SSOT docs: .claude/STATUS.md, .claude/HANDOFF.md, docs/roadmap.md, docs/ARCHITECTURE.md

YOUR JOB:
1. Verify code actually works (run it, don't assume)
2. Find bugs: broken imports, missing endpoints, orphaned data
3. Check data integrity: FK violations, orphaned links, duplicates
4. Verify STATUS.md numbers match database reality
5. Identify unused/dead code vs CLI tools (don't delete CLI tools)

OUTPUT: A prioritized list of issues with severity (critical/medium/low) and file locations.

GUARDRAILS:
- DO NOT delete files without asking
- DO NOT auto-fix - report issues, let human decide
- DO NOT create new files - update existing SSOT docs
- Internal emails (@bensley.com) are NOT bugs - they're intentionally not linked
```

### Frontend Agent

**Purpose:** Fix UI bugs, implement features, improve UX.

**Prompt:**
```
You are working on the BENSLEY Platform frontend.

CONTEXT:
- Framework: Next.js 15 (App Router) | Port: 3002
- Location: frontend/src/
- API: localhost:8000 (FastAPI backend)
- UI: shadcn/ui components + Tailwind
- Types: frontend/src/lib/types.ts
- API calls: frontend/src/lib/api.ts

YOUR JOB:
1. Read current STATUS.md and roadmap.md for priorities
2. Fix any TypeScript errors (run npm run build to check)
3. Implement requested features
4. Ensure pages load without errors

GUARDRAILS:
- DO NOT touch backend/ folder
- DO NOT create random new pages - check if similar exists first
- DO NOT change api.ts contract without checking backend supports it
- Test in browser before marking done
- Keep components simple - avoid over-engineering
```

### Backend Agent

**Purpose:** Fix API bugs, implement endpoints, optimize queries.

**Prompt:**
```
You are working on the BENSLEY Platform backend.

CONTEXT:
- Framework: FastAPI | Port: 8000
- Location: backend/api/routers/*.py (endpoints), backend/services/*.py (logic)
- Database: database/bensley_master.db (SQLite)
- Entry point: backend/api/main.py

YOUR JOB:
1. Read current STATUS.md and roadmap.md for priorities
2. Fix broken endpoints (test with curl)
3. Implement requested features
4. Ensure proper error handling

KEY TABLES:
- emails, email_content, email_proposal_links, email_project_links
- proposals, projects, contacts
- ai_suggestions (suggestion_type, status, target_table, project_code)
- learned_patterns (pattern_type, pattern_value, target_value)

GUARDRAILS:
- DO NOT touch frontend/ folder
- DO NOT create tables without checking schema first
- DO NOT auto-link emails - create suggestions for human review
- Always include project_name with project_code
- Test endpoints with curl before marking done
```

### Data Engineer Agent

**Purpose:** Fix data quality, run migrations, clean orphans.

**Prompt:**
```
You are a data engineer for the BENSLEY Platform.

CONTEXT:
- Database: database/bensley_master.db (SQLite, ~108 tables)
- Migrations: database/migrations/*.sql
- Key tables: emails (3,742), proposals (102), projects (60), contacts (546)
- Linking tables: email_proposal_links, email_project_links, proposal_contacts

YOUR JOB:
1. Run data quality queries (orphans, duplicates, FK violations)
2. Fix data issues (backfill missing values, clean orphans)
3. Verify linking integrity (every link has valid source and target)
4. Update STATUS.md with new counts after changes

GUARDRAILS:
- DO NOT drop tables without backup
- DO NOT delete data without understanding what it is
- DO NOT create new tables - check if similar exists
- Always backup before destructive operations
- Test on sample data first

COMMON QUERIES:
-- Orphaned links
SELECT COUNT(*) FROM email_proposal_links WHERE email_id NOT IN (SELECT email_id FROM emails);

-- Duplicate links
SELECT email_id, proposal_id, COUNT(*) FROM email_proposal_links GROUP BY email_id, proposal_id HAVING COUNT(*) > 1;

-- Suggestion stats
SELECT suggestion_type, status, COUNT(*) FROM ai_suggestions GROUP BY suggestion_type, status;
```

### Coordinator (You)

**Your job:** Spawn agents, reconcile their findings, update SSOT docs.

**Workflow:**
1. Check STATUS.md for current state
2. Spawn agent(s) based on task type
3. Review agent output - they may conflict or be wrong
4. Update STATUS.md with verified results
5. Create tickets for human follow-up if needed

**Key principle:** Agents work autonomously, but YOU verify before updating docs.

---

## 24. Claude-First Architecture (Dec 2025)

### Why Claude CLI, Not GPT

**GPT approach (dumb):**
```
Email arrives → GPT sees just the email text
GPT thinks: "Vahine... sounds like a hotel? Maybe link to... something?"
Result: Low confidence guess, often wrong, no business context
```

**Claude CLI approach (smart):**
```
Email arrives → Claude CLI can:
- Query: SELECT * FROM proposals WHERE project_name LIKE '%Vahine%'
- See: 25 BK-087-V is Vahine Island Resort, French Polynesia
- Check: Who else has emailed about this project?
- Know: This sender has 12 previous emails to this proposal
Result: High confidence, contextual suggestion
```

**Key insight:** GPT doesn't have database context. Claude CLI does. Claude CLI is the AI brain.

### The Architecture

```
┌─────────────────────────────────────────────────────────────────────┐
│                      EMAIL IMPORT PIPELINE                          │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  1. IMAP SYNC (automated, hourly)                                   │
│     scripts/core/scheduled_email_sync.py                            │
│     └─ Imports raw emails to `emails` table                         │
│     └─ use_gpt=False (intentionally disabled)                       │
│                                                                     │
│  2. PATTERN-FIRST LINKER (automated)                                │
│     backend/services/pattern_first_linker.py                        │
│     └─ Checks learned patterns: sender, domain, keyword, thread     │
│     └─ Auto-links if confidence >= 0.90 (70-80% of emails)          │
│     └─ Remaining emails queued for Claude analysis                  │
│                                                                     │
│  3. CLAUDE CLI ANALYSIS (interactive, with Lukas)                   │
│     └─ Claude reads unlinked emails from database                   │
│     └─ Claude has FULL context: proposals, projects, contacts       │
│     └─ Claude suggests links with reasoning                         │
│     └─ User approves/rejects                                        │
│     └─ System learns patterns from approvals                        │
│                                                                     │
│  4. PATTERN LEARNING (from Claude sessions)                         │
│     backend/services/learning_service.py                            │
│     └─ New sender → proposal pattern                                │
│     └─ New domain → proposal pattern                                │
│     └─ Confidence boosted on approval, lowered on rejection         │
│                                                                     │
│  5. FUTURE: AUTOMATED CLAUDE SCRIPT                                 │
│     scripts/core/claude_email_analyzer.py (to be built)             │
│     └─ Prepares context (proposals, contacts, patterns, history)    │
│     └─ Calls Claude API with structured prompt                      │
│     └─ Creates suggestions in ai_suggestions table                  │
│     └─ User reviews suggestions via CLI or web UI                   │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

### Current State (Dec 2025 Audit)

| Component | Status | Issue |
|-----------|--------|-------|
| Email import | ✅ Working | 161 emails in last 14 days |
| Pattern matching code | ✅ Working | 153 patterns exist |
| Pattern usage tracking | ❌ Broken | times_used never increments |
| GPT analysis | ❌ Disabled | use_gpt=False (intentional) |
| Claude CLI analysis | ⏳ Manual | Done in conversation |
| Suggestion creation | ⚠️ Minimal | Only 8 suggestions Dec 11 |
| Auto-approval | ❌ Not implemented | 0 auto-approvals |
| Learning loop | ⚠️ Partial | Patterns created but not used |

### Pattern Statistics (Dec 2025)

```
Total patterns: 153
Patterns used: 2 (1.3%)
High-confidence (0.90+): 85 patterns → 0 uses
Unlinked emails (30 days): 248
Pending suggestions: 0
```

**Root cause:** Patterns exist but aren't being applied. Pattern usage isn't tracked.

### Research Findings (Agent Audit Dec 2025)

**From ML Best Practices Research:**
- Gmail/Superhuman use pattern-first then ML fallback ✅ (we have this)
- Auto-approval threshold should be 0.90+ ✅ (code exists, not running)
- Active learning reduces labeling by 76-88%
- Need drift detection (track pattern accuracy over time)
- Batch suggestions by sender/domain to reduce fatigue

**From AI Assistant Research:**
- Three-tier approach: Pattern → AI → Human review ✅ (we have this)
- Explainability matters: show WHY a link was suggested
- Confidence calibration: claimed confidence should match actual accuracy
- Implicit feedback: track what users click vs ignore

**Key recommendation:** Claude CLI with database context > GPT without context

### Implementation Phases

**Phase 1: Fix Pattern System (Current)**
- [ ] Fix times_used tracking in pattern_first_linker.py
- [ ] Add pattern usage logging
- [ ] Enable auto-linking for 0.90+ confidence patterns
- [ ] Test pattern matching actually works

**Phase 2: Interactive Claude Sessions (Current)**
- [ ] Query unlinked emails from database
- [ ] Claude analyzes with full context
- [ ] User approves/rejects in conversation
- [ ] System learns patterns from session
- [ ] Update last_contact_date on proposals

**Phase 3: Build Automated Claude Script (Next)**
- [ ] Create scripts/core/claude_email_analyzer.py
- [ ] Build context bundler (proposals, contacts, patterns, history)
- [ ] Define structured prompt for email analysis
- [ ] Call Claude API (Anthropic) instead of GPT
- [ ] Write suggestions to ai_suggestions table
- [ ] Add to cron schedule

**Phase 4: Advanced Learning (Future)**
- [ ] Drift detection (weekly pattern accuracy report)
- [ ] Confidence calibration (adjust based on actual accuracy)
- [ ] Batch suggestions by sender/domain
- [ ] Explainability UI (show WHY suggestion was made)

### Daily Workflow with Claude CLI

**Morning routine:**
```
1. "What new emails came in overnight?"
   → Claude queries: SELECT * FROM emails WHERE date > last_session

2. "Categorize and link these emails"
   → Claude analyzes each with full DB context
   → Creates suggestions or applies links directly
   → User approves as we go

3. "What proposals need follow-up today?"
   → Claude queries: SELECT * FROM proposals WHERE last_contact_date < threshold
   → Shows context: last email, who has the ball, what was promised

4. "Update STATUS.md with today's numbers"
   → Claude runs counts, updates docs
```

### Claude Prompt for Email Analysis

When analyzing emails, Claude should:

```
For each email, check:
1. Is sender in email_learned_patterns? → Apply pattern
2. Is domain in email_learned_patterns? → Apply pattern
3. Is thread linked? → Inherit link
4. Is project code in subject? → High confidence match

If no pattern match:
1. Search proposals/projects for name matches
2. Check sender history (other emails from same sender)
3. Check domain history (other emails from same domain)
4. Look for keywords: client names, locations, project types

Suggest with confidence:
- 0.95+: Auto-apply (thread, project code, known sender)
- 0.80-0.94: Suggest with evidence
- 0.60-0.79: Suggest but flag for review
- <0.60: Log only, don't suggest

Always provide reasoning:
- "Sender joe@veyopool.com has 12 previous emails to 25 BK-069"
- "Subject contains 'Vahine' which matches 25 BK-087-V"
- "Domain @ritzcarlton.com appears in 8 emails to 25 BK-033"
```

### Scripts to Build

**1. claude_email_analyzer.py (Phase 3)**
```python
# Context builder
def build_context():
    proposals = get_active_proposals()  # With email counts, last contact
    contacts = get_contacts_with_projects()  # Contact-project mappings
    patterns = get_learned_patterns()  # Existing patterns
    return {"proposals": proposals, "contacts": contacts, "patterns": patterns}

# Email processor
def process_email(email, context):
    prompt = build_prompt(email, context)
    response = call_claude_api(prompt)
    return parse_suggestions(response)

# Main loop
for email in get_unlinked_emails():
    suggestions = process_email(email, context)
    for s in suggestions:
        create_suggestion(s)  # Write to ai_suggestions
```

**2. pattern_health_check.py (Phase 4)**
```python
# Weekly pattern accuracy check
def check_pattern_health():
    patterns = get_patterns_with_usage()
    for p in patterns:
        accuracy = p.times_correct / p.times_used
        if accuracy < 0.70:
            deactivate_pattern(p.id)
            log(f"Deactivated {p.pattern_key}: {accuracy:.0%} accuracy")
```

---

## 25. Agent Audit Findings (Dec 2025)

Five specialized agents audited the learning loop system. Complete findings below.

### Agent 1: Code Deep Dive (pattern_first_linker.py)

**Files analyzed:**
- `backend/services/pattern_first_linker.py` (757 lines)
- `backend/services/batch_suggestion_service.py` (721 lines)
- `backend/services/learning_service.py` (1,434 lines)
- `backend/services/ai_learning_service.py` (1,740 lines)

**Pattern Matching Priority Order:**
1. Thread inheritance (0.95 confidence)
2. Project code in subject `[24 BK-058]` (0.98 confidence)
3. Sender exact match (stored confidence)
4. Domain match (stored confidence)
5. Keyword match (stored confidence)
6. If no match → `needs_gpt` (but GPT is disabled)

**CRITICAL BUG FOUND:**
```python
# pattern_first_linker.apply_link() creates links BUT:
# - Never calls learning_service.log_pattern_usage()
# - Never increments times_used counter
# Result: 150/152 patterns have times_used = 0
```

**Fix required:** After `apply_link()` succeeds, add:
```python
self.execute_update("""
    UPDATE email_learned_patterns
    SET times_used = times_used + 1, last_used_at = datetime('now')
    WHERE pattern_id = ?
""", (pattern_id,))
```

### Agent 2: Pipeline Trace (scheduled_email_sync.py)

**The email processing flow:**
```
1. IMAP Sync → emails table (works ✅)
2. Pattern-First Linker → called with use_gpt=False
3. Email Orchestrator → also calls pattern linker
4. Batch Suggestion Service → creates batched suggestions
```

**WHY GPT IS DISABLED (intentional):**
```python
# scheduled_email_sync.py line 425:
process_email_links(use_gpt=False)  # Lukas will use Claude CLI instead

# line 526:
link_result = process_email_links(limit=500, use_gpt=False, db_path=DB_PATH)
```

**Result of GPT disabled:**
- Patterns that match → Auto-link ✅
- Patterns that don't match → "needs_gpt" → Nothing happens ❌
- No suggestions created → Nothing for user to review ❌
- Learning loop breaks → No human feedback ❌

**The gap:** When patterns don't match and GPT is disabled, emails sit unlinked forever.

### Agent 3: Database Analysis

**Pattern Statistics:**
| Metric | Value |
|--------|-------|
| Total patterns | 153 |
| Active patterns | 152 |
| Patterns used | 2 (1.3%) |
| High-confidence (0.90+) | 85 |
| Average confidence | 0.859 |

**Patterns by Type:**
| Type | Count | Avg Confidence | Uses |
|------|-------|----------------|------|
| sender_to_proposal | 46 | 0.847 | 0 |
| domain_to_proposal | 33 | 0.768 | 2 |
| keyword_to_proposal | 30 | 0.912 | 0 |
| keyword_to_project | 16 | 0.894 | 0 |
| domain_to_skip | 8 | 0.956 | 0 |
| sender_to_internal | 8 | 0.931 | 0 |

**The Only 2 Patterns Used:**
| Pattern | Target | Confidence | Uses |
|---------|--------|------------|------|
| parsons.com | 25 BK-042 | 0.80 | 1 |
| @landunion.com | 25 BK-086 | 0.90 | 1 |

**AI Suggestions Stats:**
| Type | Applied | Rejected | Total | Apply Rate |
|------|---------|----------|-------|------------|
| email_link | 437 | 198 | 642 | 68% |
| follow_up_needed | 120 | 74 | 195 | 62% |
| proposal_status_update | 10 | 168 | 178 | 6% |
| new_contact | 5 | 122 | 129 | 4% |
| action_required | 55 | 5 | 60 | 92% |

**Email Coverage:**
| Status | Count | % |
|--------|-------|---|
| Total emails | 3,843 | 100% |
| Linked to proposals | 2,026 | 52.7% |
| Linked to projects | 576 | 15.0% |
| Categorized (no link needed) | 207 | 5.4% |
| **Completely unlinked** | **1,330** | **34.6%** |

**Critical finding:** 1,330 emails (34.6%) are unlinked. Pattern system could auto-link many.

### Agent 4: ML Best Practices Research

**Sources researched:**
- Gmail Priority Inbox (Google Research paper)
- Superhuman email client
- HubSpot/Salesforce CRM auto-linking
- Active learning literature
- Human-in-the-loop ML surveys

**Key Findings:**

**1. Algorithm Performance on Small Datasets (1-3k samples):**
- Naive Bayes: 98.06% accuracy
- SVM: 98.00% accuracy
- Random Forest: Best for <1k samples
- **Recommendation:** Pattern-first + simple ML fallback

**2. Confidence Thresholds (Research-Backed):**
| Confidence | Action | Expected Accuracy |
|------------|--------|-------------------|
| ≥0.95 | Auto-approve | 98-99% |
| 0.85-0.94 | Batch review | 95-97% |
| 0.70-0.84 | Individual review | 85-94% |
| 0.50-0.69 | Flag for expert | 70-84% |
| <0.50 | Don't suggest | <70% |

**3. Preventing Pattern Drift:**
- Track pattern accuracy over time
- Deactivate patterns below 70% accuracy
- Reweight recent corrections higher
- Monthly calibration check

**4. Active Learning Reduces Labeling by 76-88%:**
- Uncertainty sampling: Label what model is uncertain about
- Information gain: Label what teaches the most
- Your system already does this (corrections teach patterns)

**5. Suggestion Fatigue Prevention:**
- Batch by sender/domain (15 batches vs 150 items)
- Progressive disclosure (show samples, not all)
- One-click bulk approve

### Agent 5: AI Assistant Patterns Research

**Sources researched:**
- RLHF implementations
- Few-shot learning for classification
- Explainable AI for email
- Gmail/Superhuman architecture
- CRM auto-linking systems

**Key Findings:**

**1. Three-Tier Architecture (Industry Standard):**
```
Tier 1: Pattern Matching (fast, free, 70-80%)
Tier 2: AI Analysis (when patterns fail, 15-25%)
Tier 3: Human Review (learning layer, 5-10%)
```
→ Your system has this, just needs wiring.

**2. Explainability Matters:**
Users trust and approve faster when they see WHY:
```
Link to 25 BK-087 (Vahine Island Resort)?
WHY THIS SUGGESTION:
✓ Email mentions "Vahine Island Resort" (3 times)
✓ 12 previous emails from this sender went to this project
~ New sender domain @tahaaresort.com (first time seen)
✓ Context about "contract signed Nov 21"
```

**3. Learning from Implicit Feedback:**
- User quickly approves → High confidence was correct
- User views details before approving → Was uncertain
- User ignores suggestion → Probably wrong
→ Track these signals to improve patterns

**4. Comparison to Industry:**
| Feature | Gmail | Superhuman | Your System |
|---------|-------|------------|-------------|
| Pattern matching | ✅ | ✅ | ✅ |
| ML classification | ✅ | ✅ | ⏳ (Claude) |
| Auto-approval | ✅ | ✅ | ❌ (not wired) |
| Explainability | ❌ | ⚠️ | ⏳ (planned) |
| Learning | ✅ | ✅ | ⚠️ (partial) |
| Batch review | ✅ | ✅ | ⏳ (planned) |

**5. Recommended Implementation Order:**
1. Fix pattern usage tracking (P0)
2. Enable auto-approval for 0.90+ (P0)
3. Add explainability to suggestions (P1)
4. Implement batch review (P1)
5. Add drift detection (P2)
6. Build Claude API script (P2)

### Summary: What's Broken vs What Works

**Works ✅:**
- Email import (IMAP sync)
- Pattern matching code (logic is correct)
- Database schema (tables exist)
- Suggestion handlers (apply/reject works)
- Learning service (can create patterns)

**Broken ❌:**
- Pattern usage never tracked (`times_used = 0`)
- GPT disabled, no fallback for unknown emails
- No suggestions created for unlinked emails
- Auto-approval not wired up
- 34.6% of emails sitting unlinked

**Root Cause:**
The system has all the pieces but they're disconnected:
1. Patterns exist but usage not tracked
2. GPT disabled (intentionally) but Claude not yet integrated
3. No feedback loop → patterns don't improve
